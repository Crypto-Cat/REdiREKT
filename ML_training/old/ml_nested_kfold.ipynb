{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from os import path, getcwd, system, mkdir\n",
    "from datetime import datetime\n",
    "from shutil import rmtree\n",
    "import csv\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import LSTM, Masking\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Will clear tensorflow graph (so that brand new model is created)\n",
    "tf.keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = pd.read_csv(path.join(getcwd(), \"training_data/features.csv\"))\n",
    "\n",
    "# Convert TLD to category codes\n",
    "df[\"tld\"] = df[\"tld\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Scale data between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# All features\n",
    "features_to_scale = df.copy().drop(['classification', 'sample', 'redir_no'], axis=1)\n",
    "\n",
    "# Redirect features\n",
    "#features_to_scale = df.copy().drop(['classification', 'sample', 'redir_no', 'requests_no', 'port_80', 'domain_is_ip', 'domain_len_avg', 'domain_entropy_avg', 'uri_len_avg', 'uri_entropy_avg', 'uri_ch_slash_total', 'uri_ch_slash_avg', 'uri_ch_amp_total', 'uri_ch_amp_avg','uri_ch_dash_total', 'uri_ch_dash_avg', 'uri_ch_plus_total', 'uri_ch_plus_avg', 'response_len_total', 'response_len_avg', 'bytes_shockwave_total', 'bytes_shockwave_avg', 'bytes_x-dosexec_total', 'bytes_x-dosexec_avg', 'bytes_java_total', 'bytes_java_avg', 'bytes_silverlight_total', 'bytes_silverlight_avg', 'bytes_javascript_total', 'bytes_javascript_avg', 'bytes_xml_total', 'bytes_xml_avg', 'bytes_zip_total', 'bytes_zip_avg', 'bytes_image_total', 'bytes_image_avg', 'bytes_html_total', 'bytes_html_avg', 'tld'], axis=1)\n",
    "\n",
    "# URL features\n",
    "#features_to_scale = df.copy().drop(['classification', 'sample', 'redir_no', 'redir_time', 'node_depth', 'requests_no', 'response_len_total', 'response_len_avg', 'bytes_shockwave_total', 'bytes_shockwave_avg', 'bytes_x-dosexec_total', 'bytes_x-dosexec_avg', 'bytes_java_total', 'bytes_java_avg', 'bytes_silverlight_total', 'bytes_silverlight_avg', 'bytes_javascript_total', 'bytes_javascript_avg', 'bytes_xml_total', 'bytes_xml_avg', 'bytes_zip_total', 'bytes_zip_avg', 'bytes_image_total', 'bytes_image_avg', 'bytes_html_total', 'bytes_html_avg', 'redir_referrer', 'redir_location', 'redir_html', 'redir_js', 'redir_iframe','redir_subdomain', 'redir_concat', 'redir_base64', 'redir_unknown'], axis=1)\n",
    "\n",
    "# Content features\n",
    "#features_to_scale = df.copy().drop(['classification', 'sample', 'redir_no', 'redir_time', 'node_depth', 'port_80', 'domain_is_ip', 'domain_len_avg', 'domain_entropy_avg', 'uri_len_avg', 'uri_entropy_avg', 'uri_ch_slash_total', 'uri_ch_slash_avg', 'uri_ch_amp_total', 'uri_ch_amp_avg','uri_ch_dash_total', 'uri_ch_dash_avg', 'uri_ch_plus_total', 'uri_ch_plus_avg', 'redir_referrer', 'redir_location', 'redir_html', 'redir_js', 'redir_iframe','redir_subdomain', 'redir_concat', 'redir_base64', 'redir_unknown', 'tld'], axis=1)\n",
    "\n",
    "# Content features - without totals (only averages)\n",
    "#features_to_scale = df.copy().drop(['classification', 'sample', 'redir_no', 'redir_time', 'node_depth', 'port_80', 'domain_is_ip', 'domain_len_avg', 'domain_entropy_avg', 'uri_len_avg', 'uri_entropy_avg', 'uri_ch_slash_total', 'uri_ch_slash_avg', 'uri_ch_amp_total', 'uri_ch_amp_avg','uri_ch_dash_total', 'uri_ch_dash_avg', 'uri_ch_plus_total', 'uri_ch_plus_avg', 'redir_referrer', 'redir_location', 'redir_html', 'redir_js', 'redir_iframe','redir_subdomain', 'redir_concat', 'redir_base64', 'redir_unknown', 'tld', 'response_len_total', 'bytes_shockwave_total', 'bytes_x-dosexec_total', 'bytes_java_total', 'bytes_silverlight_total',  'bytes_javascript_total', 'bytes_xml_total', 'bytes_zip_total', 'bytes_image_total', 'bytes_html_total'], axis=1)\n",
    "\n",
    "# Normalise\n",
    "normalised = pd.DataFrame(scaler.fit_transform(features_to_scale), columns=features_to_scale.columns, index=features_to_scale.index)\n",
    "\n",
    "# Rebuild normalised dataframe\n",
    "df = pd.concat([df[['classification', 'sample', 'redir_no']], normalised], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad out the groups e.g. if max number of nodes is 50, pad out each group until it has 50 rows\n",
    "df_padded = df.set_index(['sample','redir_no']).unstack(fill_value=0).stack(dropna=False).reset_index('sample')\n",
    "\n",
    "# Number of samples\n",
    "num_of_samples = len(df_padded.groupby('sample'))\n",
    "# Find the max number of nodes in any chain\n",
    "max_nodes = int(len(df_padded) / num_of_samples)\n",
    "# Number of features per chain\n",
    "features_per_node = len(df.columns) - 3 # -3 as classification + sample + redir_no will be dropped later\n",
    "\n",
    "# Assign Y to equal classification column (0/1)\n",
    "y = df_padded[['classification', 'sample']][0::max_nodes].copy() # Once every 'max_nodes'\n",
    "# Assign X to equal the remaining columns (features)\n",
    "X = df_padded.copy().drop(['classification', 'sample'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the rows: samples/time_steps/features\n",
    "X = array(X).reshape(num_of_samples, max_nodes, features_per_node)\n",
    "\n",
    "# Backup y as we may want to access sample name\n",
    "classifications = y.copy().reset_index(drop=True)\n",
    "\n",
    "# Drop the sample names from y (we only want classification)\n",
    "y = y.drop(['sample'], axis=1)\n",
    "# Convert y to numpy array so it can be processed by gridsearchcv\n",
    "y = array(y).reshape(num_of_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(neurons=1, layers=1, dropout=0.2):\n",
    "    # Create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add masking layer to ignore all timesteps where every value equals 0\n",
    "    model.add(Masking(mask_value=0., input_shape=(max_nodes, features_per_node)))\n",
    "\n",
    "    for layer in range(layers-1):\n",
    "        # Uses 'Tanh' activation function by default\n",
    "        model.add(LSTM(neurons, return_sequences=True, input_shape=(max_nodes, features_per_node))) # return_sequences true if multi-layers\n",
    "        # Add dropout to prevent overfitting\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # Final layer (don't return sequences)\n",
    "    # Uses 'Tanh' activation function by default\n",
    "    model.add(LSTM(neurons, input_shape=(max_nodes, features_per_node))) \n",
    "    # Add dropout to prevent overfitting\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # Classification problem, Dense output layer with a single neuron and sigmoid activation function to make 0/1 predictions\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Add activation layer - 'sigmoid' for binary classification (backed up by: https://www.quora.com/Why-is-it-better-to-use-Softmax-function-than-sigmoid-function)\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # Classification problem, cross entropy - https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/ \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the ML training result to CSV\n",
    "def log_result(layers, neurons, epochs, dropout, accuracy, precicion, recall, f1, rank, fit_time, test_time):\n",
    "    # If the file exists\n",
    "    if path.isfile('results/results.csv'):\n",
    "        with open ('results/results.csv','a') as f:\n",
    "            # Write results as a new row\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            # Print the new result row\n",
    "            writer.writerow([datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"), round(accuracy,3), round(precicion,3), round(recall,3), round(f1,3), len(X), test_split, val_split, neurons, layers, dropout, epochs, rank, round(fit_time,3), round(test_time,3)])\n",
    "    else:\n",
    "        # If the file doesnt exist\n",
    "        with open ('results/results.csv','w') as f:\n",
    "            # Create new CSV with following headings                       \n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerow(['date_time', 'accuracy', 'precision', 'recall', 'f1', 'data_size', 'test_folds', 'val_folds', 'neurons', 'layers', 'dropout', 'epochs', 'rank', 'fit_time', 'test_time'])\n",
    "            # Print the new result row\n",
    "            writer.writerow([datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"), round(accuracy,3), round(precicion,3), round(recall,3), round(f1,3), len(X), test_split, val_split, neurons, layers, dropout, epochs, rank, round(fit_time,3), round(test_time,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n\nAccuracy [0.99442897 0.99373259 0.99233983 0.99651811 0.99512535]\nPrecision [0.98809524 0.98804781 0.98406375 0.99209486 0.98814229]\nRecall [0.98031496 0.97637795 0.97244094 0.98818898 0.98425197]\nF1 [0.98418972 0.98217822 0.97821782 0.99013807 0.98619329]\n\nAverage Acurracy: 99.443\nAverage Precision: 98.809\nAverage Recall: 98.031\nAverage F1: 98.418\n\nAverage Fit Time: 2764.119 secs\nAverage Test Time: 0.242 secs\n\n[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 46.2min finished\n"
    }
   ],
   "source": [
    "# Val/Test splits\n",
    "test_split = 5\n",
    "val_split = 5\n",
    "\n",
    "# How many LSTM layers?\n",
    "num_of_layers = [1, 1]\n",
    "layers = list(range(num_of_layers[0], num_of_layers[1]+1))\n",
    "# layers = layers[5:] # Only want layers 5-10?\n",
    "\n",
    "# Number of hidden neurons\n",
    "num_of_nodes = [1, 1]\n",
    "neurons = list(range(num_of_nodes[0], num_of_nodes[1]+1))\n",
    "# neurons = neurons[20:] # Only want layers 5-10?\n",
    "\n",
    "# Helps prevent overfitting - typically in range 0.2-0.5 (0.x probability that each feature will be dropped)\n",
    "# Works well because model can't rely on any single feature too much (they get randomly dropped)\n",
    "dropout = [0.2]\n",
    "\n",
    "# Epoch - https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "num_of_epochs = [267, 267]\n",
    "epochs = list(range(num_of_epochs[0], num_of_epochs[1]+1))\n",
    "\n",
    "# Paramters to grid search\n",
    "param_grid = dict(neurons=neurons, layers=layers, epochs=epochs, dropout=dropout)\n",
    "\n",
    "# Statistics\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Template to build Keras Classifier (call custom build_model function)\n",
    "model = KerasClassifier(build_fn=build_model)\n",
    "\n",
    "# Perform grid search with 'val_split' folds\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=StratifiedKFold(n_splits=val_split, shuffle=True, random_state=23), n_jobs=-1, scoring=scoring, refit='f1', verbose=10)\n",
    "\n",
    "# # We don't specify batch size for sequences - https://keras.io/models/model/#fit\n",
    "# grid.fit(X, y, shuffle=True, verbose=1)\n",
    "\n",
    "# # Print best score\n",
    "# print('\\nBest parameters: ' + str(grid.best_params_))\n",
    "# print('Best score: %0.3f' % (grid.best_score_ * 100) + '%')\n",
    "\n",
    "# # Print results of grid search to CSV file\n",
    "# for i in range(len(grid.cv_results_['params'])):\n",
    "#     log_result(grid.cv_results_['params'][i]['layers'], grid.cv_results_['params'][i]['neurons'], grid.cv_results_['params'][i]['epochs'], grid.cv_results_['params'][i]['dropout'], grid.cv_results_['mean_test_accuracy'][i], grid.cv_results_['mean_test_precision'][i], grid.cv_results_['mean_test_recall'][i], grid.cv_results_['mean_test_f1'][i], grid.cv_results_['rank_test_f1'][i], grid.cv_results_['mean_fit_time'][i], grid.cv_results_['mean_score_time'][i])\n",
    "\n",
    "# Perform cross fold validation on test set\n",
    "results = cross_validate(estimator=grid, X=X, y=y, cv=StratifiedKFold(n_splits=test_split, shuffle=True, random_state=23), n_jobs=-1, scoring=scoring, verbose=1)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nAccuracy \" + str(results['test_accuracy']))\n",
    "print(\"Precision \" + str(results['test_precision']))\n",
    "print(\"Recall \" + str(results['test_recall']))\n",
    "print(\"F1 \" + str(results['test_f1']))\n",
    "\n",
    "print(\"\\nAverage Acurracy: \" + str(round(np.average(results['test_accuracy'] * 100),3)))\n",
    "print(\"Average Precision: \" + str(round(np.average(results['test_precision'] * 100),3)))\n",
    "print(\"Average Recall: \" + str(round(np.average(results['test_recall'] * 100),3)))\n",
    "print(\"Average F1: \" + str(round(np.average(results['test_f1'] * 100),3)))\n",
    "\n",
    "print(\"\\nAverage Fit Time: \" + str(round(np.average(results['fit_time']),3)) + \" secs\")\n",
    "print(\"Average Test Time: \" + str(round(np.average(results['score_time']),3)) + \" secs\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bit3d259b2695f04b1486c7122cd3c2f034",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}