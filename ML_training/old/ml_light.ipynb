{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bit3d259b2695f04b1486c7122cd3c2f034",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from datetime import datetime\n",
    "import time\n",
    "from os import path, getcwd, system, mkdir\n",
    "from shutil import rmtree\n",
    "import csv\n",
    "import scipy\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Masking\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Will clear tensorflow graph (so that brand new model is created)\n",
    "tf.keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Remove last rounds output files/stats\n",
    "system('rm -rf tests/previous/*; mv tests/current/* tests/previous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = pd.read_csv(path.join(getcwd(), \"training_data/features.csv\"))\n",
    "\n",
    "# Convert TLD to category codes\n",
    "df[\"tld\"] = df[\"tld\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Scale data between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "features_to_scale = df.copy().drop(['classification', 'sample', 'redir_no'], axis=1)\n",
    "normalised = pd.DataFrame(scaler.fit_transform(features_to_scale), columns=features_to_scale.columns, index=features_to_scale.index)\n",
    "\n",
    "# Rebuild normalised dataframe\n",
    "df = pd.concat([df[['classification', 'sample', 'redir_no']], normalised], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad out the groups e.g. if max number of nodes is 50, pad out each group until it has 50 rows\n",
    "df_padded = df.set_index(['sample','redir_no']).unstack(fill_value=0).stack(dropna=False).reset_index('sample')\n",
    "\n",
    "# Number of samples\n",
    "num_of_samples = len(df_padded.groupby('sample'))\n",
    "# Find the max number of nodes in any chain\n",
    "max_nodes = int(len(df_padded) / num_of_samples)\n",
    "# Number of features per chain\n",
    "features_per_node = len(df.columns) - 3 # -3 as classification + sample + redir_no will be dropped later\n",
    "\n",
    "# Assign Y to equal classification column (0/1)\n",
    "y = df_padded[['classification', 'sample']][0::max_nodes].copy() # Once every 'max_nodes'\n",
    "# Assign X to equal the remaining columns (features)\n",
    "X = df_padded.copy().drop(['classification', 'sample'], axis=1)\n",
    "\n",
    "# Reshape the rows: samples/time_steps/features\n",
    "X = array(X).reshape(num_of_samples, max_nodes, features_per_node)\n",
    "\n",
    "# Backup y as we may want to access sample name\n",
    "classifications = y.copy().reset_index(drop=True)\n",
    "# Drop the sample names from y (we only want classification)\n",
    "y = y.drop(['sample'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(seed):\n",
    "    # Split up the train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_set_size, random_state=seed, shuffle=True)\n",
    "\n",
    "    # Split up the train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_set_size, random_state=seed, shuffle=True)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(halve_nodes):\n",
    "    # Create a sequential model\n",
    "    model = Sequential(name=model_name)\n",
    "\n",
    "    # Add masking layer to ignore all timesteps where every value equals 0\n",
    "    model.add(Masking(mask_value=0., input_shape=(max_nodes, features_per_node)))\n",
    "\n",
    "    # Used to divide number of nodes per layer if required\n",
    "    temp_nodes = hidden_nodes\n",
    "    neurons = [] # Store the number of hidden nodes used in each layer\n",
    "\n",
    "    for layer in range(num_of_layers-1):\n",
    "        neurons.append(temp_nodes) \n",
    "        # Add LSTM layer with 'hidden_nodes' * neurons\n",
    "        # Uses 'Tanh' activation function by default\n",
    "        model.add(LSTM(temp_nodes, return_sequences=True, input_shape=(max_nodes, features_per_node))) # return_sequences true if multi-layers\n",
    "        if halve_nodes == 1:\n",
    "            # If we can still halve the temp nodes, do so\n",
    "            if int(temp_nodes / 2) >= 1:\n",
    "                # Halve temp nodes\n",
    "                temp_nodes = int(temp_nodes / 2)\n",
    "        # Add dropout to prevent overfitting\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # Final layer (don't return sequences)\n",
    "    # Uses 'Tanh' activation function by default\n",
    "    model.add(LSTM(temp_nodes, input_shape=(max_nodes, features_per_node))) \n",
    "    # Add the final neurons value\n",
    "    neurons.append(temp_nodes)\n",
    "    # Add dropout to prevent overfitting\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # Classification problem, Dense output layer with a single neuron and sigmoid activation function to make 0/1 predictions\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Add activation layer - 'sigmoid' for binary classification (backed up by: https://www.quora.com/Why-is-it-better-to-use-Softmax-function-than-sigmoid-function)\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # Classification problem, cross entropy - https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/ \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Early stopping can be used to interupt training when the best validation loss has not improved for 'patience_no' epochs\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience_no, restore_best_weights=True)\n",
    "\n",
    "    # Model checkpoint will ensure only the best model is saved (every patience_no epochs)\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=test_dir + 'model.h5', monitor='val_loss', period=1, save_best_only=True, save_weights_only=False)\n",
    "\n",
    "    # Print the model summary\n",
    "    # model.summary()\n",
    "\n",
    "    # We don't specify batch size for sequences - https://keras.io/models/model/#fit\n",
    "    history = model.fit(X_train, y_train, validation_data=[X_val, y_val], epochs=epochs_no, callbacks=[early_stopping, model_checkpoint], shuffle=True, verbose=0)\n",
    "\n",
    "    # If early stopping didn't occur..\n",
    "    if early_stopping.stopped_epoch == 0:\n",
    "        model = load_model(test_dir + 'model.h5')\n",
    "        best_epoch = epochs_no - 1\n",
    "        stopped_epoch = epochs_no - 1\n",
    "    else:\n",
    "        # Set the model to equal the best model we found during training\n",
    "        model = early_stopping.model\n",
    "        best_epoch = early_stopping.stopped_epoch - patience_no\n",
    "        stopped_epoch = early_stopping.stopped_epoch\n",
    "\n",
    "    # Final evaluation of the model\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # Lets get some stats\n",
    "    stats = {\n",
    "        'test_acc' : round(results[1]*100,3),\n",
    "        'test_loss' : round(results[0]*100,3),\n",
    "        'train_acc_best' : round(history.history['accuracy'][best_epoch]*100,3),\n",
    "        'train_acc_avg' : round(np.average(history.history['accuracy'])*100,3),\n",
    "        'train_loss_best' : round(history.history['loss'][best_epoch]*100,3),\n",
    "        'train_loss_avg' : round(np.average(history.history['loss'])*100,3),\n",
    "        'val_acc_best' : round(history.history['val_accuracy'][best_epoch]*100,3),\n",
    "        'val_acc_avg' : round(np.average(history.history['val_accuracy'])*100,3),\n",
    "        'val_loss_best' : round(history.history['val_loss'][best_epoch]*100,3),\n",
    "        'val_loss_avg' : round(np.average(history.history['val_loss'])*100,3),\n",
    "        'neurons_per_layer': neurons,\n",
    "        'best_epoch': best_epoch,\n",
    "        'stopped_epoch': stopped_epoch\n",
    "    }\n",
    "\n",
    "    # Print test stats\n",
    "    print(\"=================================\\n\" + str(num_of_layers) + \" Layers, \" + str(stats['neurons_per_layer']) + \" Nodes\")\n",
    "    print(\"Best Epoch: \" + str(stats['best_epoch']))\n",
    "    print(\"Validation Accuracy: %.2f%%\" % (stats['val_acc_best']))\n",
    "    print(\"Validation Loss: %.2f%%\" % (stats['val_loss_best']))\n",
    "    print(\"Test Accuracy: %.2f%%\" % (stats['test_acc']))\n",
    "    print(\"Test Loss: %.2f%%\\n\" % (stats['test_loss']))\n",
    "\n",
    "    return stats, model, results, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the ML training result to CSV\n",
    "def log_result():\n",
    "    # If the file exists\n",
    "    if path.isfile('results/results.csv'):\n",
    "        with open ('results/results.csv','a') as f:\n",
    "            # Write results as a new row\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerow([datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"), stats['test_acc'], stats['test_loss'], stats['val_acc_best'], stats['val_acc_avg'], stats['val_loss_best'], stats['val_loss_avg'], stats['train_acc_best'], stats['train_acc_avg'], stats['train_loss_best'], stats['train_loss_avg'], len(y_train), len(y_val), len(y_test), hidden_nodes, stats['neurons_per_layer'], num_of_layers, patience_no, dropout, epochs_no, stats['stopped_epoch'], stats['best_epoch'], stats['train_time']])\n",
    "    else:\n",
    "        # If the file doesnt exist\n",
    "        with open ('results/results.csv','w') as f:     \n",
    "            # Create new CSV with following headings                       \n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerow(['date_time', 'test_acc', 'test_loss', 'val_acc_best', 'val_acc_avg', 'val_loss_best', 'val_loss_avg', 'train_acc_best', 'train_acc_avg', 'train_loss_best', 'train_loss_avg', 'train_size', 'val_size' , 'test_size', 'max_neurons', 'neurons_per_layer', 'layers', 'patience', 'dropout', 'max_epochs', 'stopped_epoch', 'best_epoch', 'train_time'])\n",
    "            # Print the new result row\n",
    "            writer.writerow([datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"), stats['test_acc'], stats['test_loss'], stats['val_acc_best'], stats['val_acc_avg'], stats['val_loss_best'], stats['val_loss_avg'], stats['train_acc_best'], stats['train_acc_avg'], stats['train_loss_best'], stats['train_loss_avg'], len(y_train), len(y_val), len(y_test), hidden_nodes, stats['neurons_per_layer'], num_of_layers, patience_no, dropout, epochs_no, stats['stopped_epoch'], stats['best_epoch'], stats['train_time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of test set\n",
    "test_set_size = 0.15\n",
    "# This is actually 0.15 of overall data but because the test set has already been removed, must increase to 0.1765\n",
    "val_set_size = 0.1765 # 0.1764705882352941 x 0.85 = 0.15\n",
    "\n",
    "# Number of units - https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm\n",
    "# https://www.researchgate.net/post/How_should_I_choose_the_optimum_number_for_the_neurons_in_the_input_hidden_layer_for_a_recurrent_neural_network \n",
    "# https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046 \n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# TODO: Tried this formula as per one of links above had better results using max_nodes\n",
    "# hidden_nodes = int(2/3 * (max_nodes * features_per_node))\n",
    "hidden_nodes_loop = [0,50] # [0,100] == 1-100, [49,100] == 50-100, [1,2] == 2\n",
    "\n",
    "# Helps prevent overfitting - typically in range 0.2-0.5 (0.x probability that each feature will be dropped)\n",
    "# Works well because model can't rely on any single feature too much (they get randomly dropped)\n",
    "dropout = 0.2\n",
    "\n",
    "# How many LSTM layers?\n",
    "num_of_layers_loop = [0,5] # [0,1] == 1 layer, [1,2] == 2 layer, [0,2] == 1+2 layer etc.\n",
    "\n",
    "# Epoch - https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/ \n",
    "epochs_no = 500\n",
    "\n",
    "# Patience is used in earlystopping/modelcheckpoint - Has the model improved in last 'patience_no' epochs?\n",
    "patience_no = 100\n",
    "\n",
    "# Can use this to divide hidden nodes as number of layers increase (HALF-NODES)\n",
    "# e.g. for 3 layers, 40 nodes: first layer has 40 nodes, second layer has 20, third layer has 10\n",
    "half_nodes_loop = [0,1] # change between [0,1], [1,2] and [0,2] for division of layers (dont divide, divide, both)\n",
    "\n",
    "# How many iterations of the same config\n",
    "iterations = 3\n",
    "\n",
    "# Change this to range of whatever variable we want to iterate over\n",
    "# Or add embedded loops to iterate over multiple variables\n",
    "for i in range(0, iterations):\n",
    "    # Create a new training/validation/test distribution\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(i + 1)\n",
    "    for j in range(num_of_layers_loop[0], num_of_layers_loop[1]): \n",
    "        num_of_layers = j + 1\n",
    "        for k in range(hidden_nodes_loop[0], hidden_nodes_loop[1]): # change the 0 if only testing 1 layer\n",
    "            hidden_nodes = k + 1\n",
    "            for l in range(half_nodes_loop[0], half_nodes_loop[1]):\n",
    "                # If using halve_nodes, we only want to process if we can divide the nodes in half\n",
    "                # And, if there is more than 1 layer\n",
    "                if (l == 1 and int(hidden_nodes / 2) > 1 and num_of_layers > 1) or l == 0:\n",
    "\n",
    "                    # Create the test directory\n",
    "                    model_name = str(num_of_layers) + '_layers_' + str(hidden_nodes) + '_nodes_' + str(l) + '_halve_' + str(i)\n",
    "                    test_dir = 'tests/current/' + model_name + \"/\"\n",
    "                    mkdir(test_dir)\n",
    "\n",
    "                    t_start = time.time() # Track time of modelling\n",
    "                    stats, model, results, history = create_model(l) # Create and run the model\n",
    "                    stats['train_time'] = round((time.time() - t_start),3) # Store time taken\n",
    "\n",
    "                    log_result() # Log statistics of this result to CSV\n",
    "\n",
    "                    tf.keras.backend.clear_session() # Clear any ML stuff\n",
    "                    del stats, model, results, history # Clear variables\n",
    "                    rmtree(test_dir) # Remove the test directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}